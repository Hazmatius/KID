# KID
KID is an effort by me ([Alex Baranski](https://www.linkedin.com/in/alex-baranski-082100a1/)) to construct a robotic arm. I think that one of the major limits of a system is that systems ability to interact with its environment through its sensory and motor apparatuses, and that simulation of an environment for training a system is [1] prone to leaving out important but hard to simulate dynamics that exist in the real world and [2] is less cool. I looked around for a robotic arm I could buy, but there's pretty much nothing in my price range that's any better than a simple claw, which I'm not interested in. So, I decided to just build my own! This is one of several things I'm juggling so new posts are infrequent, but at the end I'll make a single comprehensive tutorial with links to materials and digital resources used in the construction of the "final" iteration.
Because I'm making this from scratch, I have to learn a lot as I go, which is pretty fun. Because I'm also funding it myself, it also has to be cheap. And because I'd like to be able to iterate quickly, rapid manufacturing with limited tools is non-negotiable. I think making the whole arm low-cost is an interesting creative limitation, and I hope will make robotics more accessible than it currently is.

I have some details about the hardware at [this website](https://sites.google.com/s/1cxzKrNBqjHTFoiHB7bzBj2pTloyNXd0s/p/1--FoMWqyCzP7gzcQ5LMKvzVR4WBZFQIs/edit)

## Dataset
If I knew anything about physics, and the hand design I was using had actual joints instead of some fishing line pulling on plastic connected by duct-tape, I could probably just write a kinematics model. But I haven't taken physics in almost five years, and the hand is in fact just some plastic connected by duct-tape getting pulled around by some fishing line. So instead, I've decided to train a neural network to model the physics of the hand.
If you know anything about neural networks, you know they need data. So, I took two of the fingers off of KID (I only  wanted to have to deal with control of a single finger for the time being) and set it up on a stand in the corner of a room with white walls and relatively uniform lighting. Then I wrote some code to generate random (but continuous, the randomness was in the derivative of the commands, so KID wouldn't get jolted around. I found it helped generate smoother motions) and simultaneously take pictures of the finger. This is an example of what's called motor-babbling. Motor-babbling is when you randomly move around and just see how your sensors change. It's not a terribly sophisticated way to learn about your body, but it IS very easy to implement, so that's what I did. I collected about 10,000 data points, saved it as a folder of images with each image named by the motor commands that were sent during that time-frame, as well as a timestamp to help order the frames.

Then I wrote a little neural network in PyTorch with a dataloader that could deliver the image of the finger, motor commands, and timestamp for a single instant in time, as well as the previous and following instants. Basically, each datapoint looks like the image below. For a given timestamp *t*, there is a sensor state vector *s<sub>t</sub>* that corresponds to the image taken at that instant, as well as an action vector *a<sub>t</sub>* that corresponds to the motor commands sent at that instant. We also get the previous state and action and the following state and action, marked as (*s<sub>t-1</sub>*, *a<sub>t-1</sub>*) and (*s<sub>t+1</sub>*, *a<sub>t+1</sub>*), respectively. The dataloader at this point has already sorted through everything and knows how to just grab these things irrespective the timestamp, so it can assemble a batch of random time points.  

*Figure 1*
![what a single sample looks like](diagrams/dataset_format.JPG)

## Neural Network
My first, basic task was to predict the next state. Throughout the rest of this description, I'll be providing diagrams to help clarify what I'm saying. I've adopted a particular notation to simplify these diagrams, which has evolved over the last few years as I incessantly doodle these things for myself. In detailed fashion, I'd use a diagram like what's to the top left to represent that that there are two variables, *s<sub>t</sub>* and *s<sub>t+1</sub>*, and that there is some neural network that takes as input *s<sub>t</sub>* and outputs a prediction for *s<sub>t+1</sub>*, indicated by *~s<sub>t+1</sub>* in the diagram. We enforce this approximating relationship by calculating an error signal marked by epsilon that represents some sort of difference between *s<sub>t+1</sub>* and *~s<sub>t+1</sub>*. We then take that error signal and use backpropagation to modify the neural network, somewhat clumsily represented by the flat-headed arrow going from epsilon to the neural network arrow (this often gets used to mark inhibitory signals in other fields, so I'm definitely abusing diagramatic notation here). I personally find this all somewhat cluttered, so the diagramatic shorthand I use to represent this situation is show in the bottom right. The backwards arrow I use through the following descriptions to indicate the situation that a neural network takes in *s<sub>t</sub>* and tries to predict *s<sub>t+1</sub>*.
![diagramatic notation, complete](diagrams/diagramatic_notation_1.JPG) ![diagramatic notation, simplified](diagrams/diagramatic_notation_2.JPG)

Now, the above goal of predicting the next state of the finger from the previous state of the finger is strictly impossible, because the next state of the finger st+1 isn't uniquely determined by only the current state of the finger st, but rather by both st and the current motor commands at. The neural network we get out of this arrangement looks like what's to the top right. Now, the sensor state vector is pretty huge, I forget the exact size but on the order of a thousand dimensions because it's an actual image of the finger. I happen to know that a lot of these dimensions are redundant, because each individual finger only has three degrees of freedom, and most of the image is blank space behind the finger anyway. Since more dimensions between fully connected layers means more parameters to tune during backpropagation, more dimensions means longer training time and higher chance of overfitting. So, I decided to jointly learn a compressed representation rt of each state vector st, and then predict rt+1 from at and rt. The compression from st to rt is achieved with a convolutional autoencoder with a fully connected end that encodes the image st into a new, much smaller state vector rt. This allows the fully connected layers from at and rt to rt+1 to be much smaller, which makes training a lot faster. I tried training this model on my dataset, but ran into a major problem. My earlier assumption that you could uniquely determine the next camera image from the current camera image and current actuator commands was incorrect! The problem was that for any given position the finger was in, each single motor (which spins continuously) could be in any of two states, since the position of the finger is governed entirely by the vertical displacement of lever arm attached to the motor, but is independent of the cosine of the angle of the lever arm. 
